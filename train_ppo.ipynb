{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cad9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c8c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device to cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if(torch.cuda.is_available()): \n",
    "    device = torch.device('cuda:0') \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")\n",
    "    \n",
    "print(\"============================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af74b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.choose_jurisdiction import *\n",
    "from ipynb.fs.full.step_function import *\n",
    "from ipynb.fs.full.PPO_algo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7573184a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7667f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Training ###################################\n",
    "\n",
    "\n",
    "####### initialize environment hyperparameters ######\n",
    "\n",
    "action_std_decay_rate = 0.0046\n",
    "min_action_std = 0.05\n",
    "action_std_decay_freq = 1000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "has_continuous_action_space = True\n",
    "\n",
    "max_ep_len = 12                   # max timesteps in one episode\n",
    "max_training_timesteps = 100000   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "print_freq = max_ep_len * 10     # print avg reward in the interval (in num timesteps)\n",
    "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
    "save_model_freq = 2000      # save model frequency (in num timesteps)\n",
    "plot_freq = 1200\n",
    "\n",
    "action_std = 0.4\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "## Note : print/log frequencies should be > than max_ep_len\n",
    "\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "\n",
    "\n",
    "update_timestep = 120     # update policy every n timesteps\n",
    "K_epochs = 20               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma_ = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.0003       # learning rate for critic network\n",
    "\n",
    "random_seed = 10   # set random seed if required (0 = no random seed)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "env_name = 'HIV Jurisdiction'\n",
    "\n",
    "print(\"training environment name : \" + env_name)\n",
    "\n",
    "# env = gym.make(env_name)\n",
    "\n",
    "# state space dimension\n",
    "obs_dim = 15\n",
    "state_dim = 120\n",
    "\n",
    "# action space dimension\n",
    "if has_continuous_action_space:\n",
    "    action_dim = 9\n",
    "else:\n",
    "    action_dim = 1\n",
    "    \n",
    "###################### logging ######################\n",
    "\n",
    "#### log files for multiple runs are NOT overwritten\n",
    "\n",
    "log_dir = \"PPO_logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "log_dir = log_dir + '/' + env_name + '/'\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "\n",
    "#### get number of log files in log directory\n",
    "run_num = 0\n",
    "current_num_files = next(os.walk(log_dir))[2]\n",
    "run_num = len(current_num_files)\n",
    "\n",
    "\n",
    "#### create new log file for each run \n",
    "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
    "\n",
    "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
    "print(\"logging at : \" + log_f_name)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "################### checkpointing ###################\n",
    "\n",
    "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
    "\n",
    "directory = \"PPO_preTrained\"\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "directory = directory + '/' + env_name + '/' \n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "\n",
    "checkpoint_path1 = directory + 'Cluster1' + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "checkpoint_path2 = directory + 'Cluster2' + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "checkpoint_path3 = directory + 'Cluster3' + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "checkpoint_path4 = directory + 'Cluster4' + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "checkpoint_path5 = directory + 'Cluster5' + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "checkpoint_path6 = directory + 'Cluster6' + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "checkpoint_path7 = directory + 'Cluster7' + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "checkpoint_path8 = directory + 'Cluster8' + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "\n",
    "print(\"save checkpoint path : \" + checkpoint_path1)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "############# print all hyperparameters #############\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"max training timesteps : \", max_training_timesteps)\n",
    "print(\"max timesteps per episode : \", max_ep_len)\n",
    "\n",
    "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
    "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
    "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"observation space dimension : \", obs_dim)\n",
    "print(\"state space dimension : \", state_dim)\n",
    "print(\"action space dimension : \", action_dim)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "if has_continuous_action_space:\n",
    "    print(\"Initializing a continuous action space policy\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"starting std of action distribution : \", action_std)\n",
    "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
    "    print(\"minimum std of action distribution : \", min_action_std)\n",
    "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
    "\n",
    "else:\n",
    "    print(\"Initializing a discrete action space policy\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\") \n",
    "print(\"PPO K epochs : \", K_epochs)\n",
    "print(\"PPO epsilon clip : \", eps_clip)\n",
    "print(\"discount factor (gamma_) : \", gamma_)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"optimizer learning rate actor : \", lr_actor)\n",
    "print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "if random_seed:\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"setting random seed to \", random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "#     env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "print(\"============================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# training procedure ################\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent1 = PPO(obs_dim, state_dim, action_dim, lr_actor, lr_critic, gamma_, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "ppo_agent2 = PPO(obs_dim, state_dim, action_dim, lr_actor, lr_critic, gamma_, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "ppo_agent3 = PPO(obs_dim, state_dim, action_dim, lr_actor, lr_critic, gamma_, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "ppo_agent4 = PPO(obs_dim, state_dim, action_dim, lr_actor, lr_critic, gamma_, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "ppo_agent5 = PPO(obs_dim, state_dim, action_dim, lr_actor, lr_critic, gamma_, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "ppo_agent6 = PPO(obs_dim, state_dim, action_dim, lr_actor, lr_critic, gamma_, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "ppo_agent7 = PPO(obs_dim, state_dim, action_dim, lr_actor, lr_critic, gamma_, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "ppo_agent8 = PPO(obs_dim, state_dim, action_dim, lr_actor, lr_critic, gamma_, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "# track total training time\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "# logging file\n",
    "log_f = open(log_f_name,\"w+\")\n",
    "log_f.write('episode,timestep,reward\\n')\n",
    "\n",
    "# rew_list = []\n",
    "rew_list1 = []\n",
    "rew_list2 = []\n",
    "rew_list3 = []\n",
    "rew_list4 = []\n",
    "rew_list5 = []\n",
    "rew_list6 = []\n",
    "rew_list7 = []\n",
    "rew_list8 = []\n",
    "\n",
    "ep_rew_list1 = []\n",
    "ep_rew_list2 = []\n",
    "ep_rew_list3 = []\n",
    "ep_rew_list4 = []\n",
    "ep_rew_list5 = []\n",
    "ep_rew_list6 = []\n",
    "ep_rew_list7 = []\n",
    "ep_rew_list8 = []\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward1 = 0\n",
    "print_running_reward2 = 0\n",
    "print_running_reward3 = 0\n",
    "print_running_reward4 = 0\n",
    "print_running_reward5 = 0\n",
    "print_running_reward6 = 0\n",
    "print_running_reward7 = 0\n",
    "print_running_reward8 = 0\n",
    "\n",
    "print_running_episodes = 0\n",
    "\n",
    "log_running_reward = 0\n",
    "log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "\n",
    "# ESS_SOC_max = 1500\n",
    "# T = 24\n",
    "# training loop\n",
    "while time_step <= max_training_timesteps:\n",
    "    \n",
    "    state = initial_state(data_array_cluster,prep_values)\n",
    "\n",
    "    current_ep_reward1 = 0\n",
    "    current_ep_reward2 = 0\n",
    "    current_ep_reward3 = 0\n",
    "    current_ep_reward4 = 0\n",
    "    current_ep_reward5 = 0\n",
    "    current_ep_reward6 = 0\n",
    "    current_ep_reward7 = 0\n",
    "    current_ep_reward8 = 0\n",
    "       \n",
    "    for t in range(0, max_ep_len+1):\n",
    "        \n",
    "        # select action with policy\n",
    "        \n",
    "        full_state = np.vstack((state[1],state[2],state[3],state[4],state[5],state[6],state[7],state[8])).flatten()\n",
    "\n",
    "        action1 = ppo_agent1.select_action(state[1].flatten())\n",
    "        action2 = ppo_agent2.select_action(state[2].flatten())\n",
    "        action3 = ppo_agent3.select_action(state[3].flatten())\n",
    "        action4 = ppo_agent4.select_action(state[4].flatten())\n",
    "        action5 = ppo_agent5.select_action(state[5].flatten())\n",
    "        action6 = ppo_agent6.select_action(state[6].flatten())\n",
    "        action7 = ppo_agent7.select_action(state[7].flatten())\n",
    "        action8 = ppo_agent8.select_action(state[8].flatten())   \n",
    "\n",
    "\n",
    "        state,reward1,reward2,reward3,reward4,reward5,reward6,reward7,reward8,done  = step(state, action1, action2, action3, action4, action5, action6, action7, action8)\n",
    "\n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent1.buffer.full_states.append(torch.FloatTensor(full_state).to(device))\n",
    "        ppo_agent1.buffer.actions_others.append(torch.FloatTensor(np.hstack((action2,action3,action4,action5,action6,action7,action8))).to(device))\n",
    "        ppo_agent1.buffer.rewards.append(reward1)\n",
    "        ppo_agent1.buffer.is_terminals.append(done)\n",
    "        \n",
    "        ppo_agent2.buffer.full_states.append(torch.FloatTensor(full_state).to(device))\n",
    "        ppo_agent2.buffer.actions_others.append(torch.FloatTensor(np.hstack((action1,action3,action4,action5,action6,action7,action8))).to(device))\n",
    "        ppo_agent2.buffer.rewards.append(reward2)\n",
    "        ppo_agent2.buffer.is_terminals.append(done)\n",
    "        \n",
    "        ppo_agent3.buffer.full_states.append(torch.FloatTensor(full_state).to(device))\n",
    "        ppo_agent3.buffer.actions_others.append(torch.FloatTensor(np.hstack((action1,action2,action4,action5,action6,action7,action8))).to(device))\n",
    "        ppo_agent3.buffer.rewards.append(reward3)\n",
    "        ppo_agent3.buffer.is_terminals.append(done)\n",
    "        \n",
    "        ppo_agent4.buffer.full_states.append(torch.FloatTensor(full_state).to(device))\n",
    "        ppo_agent4.buffer.actions_others.append(torch.FloatTensor(np.hstack((action1,action2,action3,action5,action6,action7,action8))).to(device))\n",
    "        ppo_agent4.buffer.rewards.append(reward4)\n",
    "        ppo_agent4.buffer.is_terminals.append(done)\n",
    "        \n",
    "        ppo_agent5.buffer.full_states.append(torch.FloatTensor(full_state).to(device))\n",
    "        ppo_agent5.buffer.actions_others.append(torch.FloatTensor(np.hstack((action1,action2,action3,action4,action6,action7,action8))).to(device))\n",
    "        ppo_agent5.buffer.rewards.append(reward5)\n",
    "        ppo_agent5.buffer.is_terminals.append(done)\n",
    "        \n",
    "        ppo_agent6.buffer.full_states.append(torch.FloatTensor(full_state).to(device))\n",
    "        ppo_agent6.buffer.actions_others.append(torch.FloatTensor(np.hstack((action1,action2,action3,action4,action5,action7,action8))).to(device))\n",
    "        ppo_agent6.buffer.rewards.append(reward6)\n",
    "        ppo_agent6.buffer.is_terminals.append(done)\n",
    "        \n",
    "        ppo_agent7.buffer.full_states.append(torch.FloatTensor(full_state).to(device))\n",
    "        ppo_agent7.buffer.actions_others.append(torch.FloatTensor(np.hstack((action1,action2,action3,action4,action5,action6,action8))).to(device))\n",
    "        ppo_agent7.buffer.rewards.append(reward7)\n",
    "        ppo_agent7.buffer.is_terminals.append(done)\n",
    "        \n",
    "        ppo_agent8.buffer.full_states.append(torch.FloatTensor(full_state).to(device))\n",
    "        ppo_agent8.buffer.actions_others.append(torch.FloatTensor(np.hstack((action1,action2,action3,action4,action5,action6,action7))).to(device))\n",
    "        ppo_agent8.buffer.rewards.append(reward8)\n",
    "        ppo_agent8.buffer.is_terminals.append(done)\n",
    "\n",
    "        \n",
    "        time_step +=1\n",
    "#         ltc_risk += ltc_increment\n",
    "        current_ep_reward1 += reward1\n",
    "        current_ep_reward2 += reward2\n",
    "        current_ep_reward3 += reward3\n",
    "        current_ep_reward4 += reward4\n",
    "        current_ep_reward5 += reward5\n",
    "        current_ep_reward6 += reward6\n",
    "        current_ep_reward7 += reward7\n",
    "        current_ep_reward8 += reward8\n",
    "\n",
    "        \n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent1.update()\n",
    "            ppo_agent2.update()\n",
    "            ppo_agent3.update()\n",
    "            ppo_agent4.update()\n",
    "            ppo_agent5.update()\n",
    "            ppo_agent6.update()\n",
    "            ppo_agent7.update()\n",
    "            ppo_agent8.update()\n",
    "\n",
    "\n",
    "        # if continuous action space; then decay action std of ouput action distribution\n",
    "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
    "            ppo_agent1.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "            ppo_agent2.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "            ppo_agent3.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "            ppo_agent4.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "            ppo_agent5.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "            ppo_agent6.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "            ppo_agent7.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "            ppo_agent8.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "\n",
    "        # log in logging file\n",
    "        if time_step % log_freq == 0:\n",
    "\n",
    "            # log average reward till last episode\n",
    "            log_avg_reward = log_running_reward / log_running_episodes\n",
    "            log_avg_reward = round(log_avg_reward, 4)\n",
    "\n",
    "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
    "            log_f.flush()\n",
    "\n",
    "            log_running_reward = 0\n",
    "            log_running_episodes = 0\n",
    "\n",
    "        # printing average reward\n",
    "        if time_step % print_freq == 0:\n",
    "\n",
    "            # print average reward till last episode\n",
    "            print_avg_reward1 = print_running_reward1 / print_running_episodes\n",
    "            print_avg_reward1 = round(print_avg_reward1, 2)\n",
    "            \n",
    "            print_avg_reward2 = print_running_reward2 / print_running_episodes\n",
    "            print_avg_reward2 = round(print_avg_reward2, 2)\n",
    "            \n",
    "            print_avg_reward3 = print_running_reward3 / print_running_episodes\n",
    "            print_avg_reward3 = round(print_avg_reward3, 2)\n",
    "\n",
    "            print_avg_reward4 = print_running_reward4 / print_running_episodes\n",
    "            print_avg_reward4 = round(print_avg_reward4, 2)\n",
    "            \n",
    "            print_avg_reward5 = print_running_reward5 / print_running_episodes\n",
    "            print_avg_reward5 = round(print_avg_reward5, 2)\n",
    "            \n",
    "            print_avg_reward6 = print_running_reward6 / print_running_episodes\n",
    "            print_avg_reward6 = round(print_avg_reward6, 2)\n",
    "\n",
    "            print_avg_reward7 = print_running_reward7 / print_running_episodes\n",
    "            print_avg_reward7 = round(print_avg_reward7, 2)\n",
    "            \n",
    "            print_avg_reward8 = print_running_reward8 / print_running_episodes\n",
    "            print_avg_reward8 = round(print_avg_reward8, 2)\n",
    "            \n",
    "\n",
    "            print(\"Agent1 => Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward1))\n",
    "            print(\"Agent2 => Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward2))\n",
    "            print(\"Agent3 => Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward3))\n",
    "            print(\"Agent4 => Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward4))\n",
    "            print(\"Agent5 => Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward5))\n",
    "            print(\"Agent6 => Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward6))\n",
    "            print(\"Agent7 => Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward7))\n",
    "            print(\"Agent8 => Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward8))\n",
    "           \n",
    "            rew_list1.append(print_avg_reward1)\n",
    "            rew_list2.append(print_avg_reward2)\n",
    "            rew_list3.append(print_avg_reward3)\n",
    "            rew_list4.append(print_avg_reward4)\n",
    "            rew_list5.append(print_avg_reward5)\n",
    "            rew_list6.append(print_avg_reward6)\n",
    "            rew_list7.append(print_avg_reward7)\n",
    "            rew_list8.append(print_avg_reward8)\n",
    "           \n",
    "            print_running_reward1 = 0\n",
    "            print_running_reward2 = 0\n",
    "            print_running_reward3 = 0\n",
    "            print_running_reward4 = 0\n",
    "            print_running_reward5 = 0\n",
    "            print_running_reward6 = 0\n",
    "            print_running_reward7 = 0\n",
    "            print_running_reward8 = 0\n",
    "  \n",
    "\n",
    "            print_running_episodes = 0\n",
    "            \n",
    "        if time_step % plot_freq == 0:\n",
    "            fig,ax = plt.subplots(2,4,sharex=False, sharey=False, figsize=(20,7))\n",
    "            fig.tight_layout(h_pad=3, w_pad=1)\n",
    "            \n",
    "            ax[0][0].plot(range(len(rew_list1)), rew_list1)\n",
    "            ax[0][0].set_title('Rewards for Cluster 1',pad=12)\n",
    "            ax[0][1].plot(range(len(rew_list2)), rew_list2)\n",
    "            ax[0][1].set_title('Rewards for Cluster 2',pad=12)\n",
    "            ax[0][2].plot(range(len(rew_list3)), rew_list3)\n",
    "            ax[0][2].set_title('Rewards for Cluster 3',pad=12)\n",
    "            ax[0][3].plot(range(len(rew_list4)), rew_list4)\n",
    "            ax[0][3].set_title('Rewards for Cluster 4',pad=12)\n",
    "            ax[1][0].plot(range(len(rew_list5)), rew_list5)\n",
    "            ax[1][0].set_title('Rewards for Cluster 5',pad=12)\n",
    "            ax[1][1].plot(range(len(rew_list6)), rew_list6)\n",
    "            ax[1][1].set_title('Rewards for Cluster 6',pad=12)\n",
    "            ax[1][2].plot(range(len(rew_list7)), rew_list7)\n",
    "            ax[1][2].set_title('Rewards for Cluster 7',pad=12)\n",
    "            ax[1][3].plot(range(len(rew_list8)), rew_list8)\n",
    "            ax[1][3].set_title('Rewards for Cluster 8',pad=12)\n",
    "\n",
    "            \n",
    "#             fig.suptitle('Rewards for all the agents')\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "        # save model weights\n",
    "        if time_step % save_model_freq == 0:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"saving model at : \" + checkpoint_path1)\n",
    "#             print(\"saving model at : \" + checkpoint_path2)\n",
    "#             print(\"saving model at : \" + checkpoint_path3)\n",
    "            ppo_agent1.save(checkpoint_path1)\n",
    "            ppo_agent2.save(checkpoint_path2)\n",
    "            ppo_agent3.save(checkpoint_path3)\n",
    "            ppo_agent4.save(checkpoint_path4)\n",
    "            ppo_agent5.save(checkpoint_path5)\n",
    "            ppo_agent6.save(checkpoint_path6)\n",
    "            ppo_agent7.save(checkpoint_path7)\n",
    "            ppo_agent8.save(checkpoint_path8)\n",
    "\n",
    "            print(\"model saved\")\n",
    "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            \n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "\n",
    "    print_running_reward1 += current_ep_reward1\n",
    "    print_running_reward2 += current_ep_reward2\n",
    "    print_running_reward3 += current_ep_reward3\n",
    "    print_running_reward4 += current_ep_reward4\n",
    "    print_running_reward5 += current_ep_reward5\n",
    "    print_running_reward6 += current_ep_reward6\n",
    "    print_running_reward7 += current_ep_reward7\n",
    "    print_running_reward8 += current_ep_reward8\n",
    "    \n",
    "    ep_rew_list1.append(current_ep_reward1)\n",
    "    ep_rew_list2.append(current_ep_reward2)\n",
    "    ep_rew_list3.append(current_ep_reward3)\n",
    "    ep_rew_list4.append(current_ep_reward4)\n",
    "    ep_rew_list5.append(current_ep_reward5)\n",
    "    ep_rew_list6.append(current_ep_reward6)\n",
    "    ep_rew_list7.append(current_ep_reward7)\n",
    "    ep_rew_list8.append(current_ep_reward8)\n",
    "\n",
    "    print_running_episodes += 1\n",
    "\n",
    "    log_running_reward += current_ep_reward1\n",
    "    log_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n",
    "\n",
    "\n",
    "log_f.close()\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d6b52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
